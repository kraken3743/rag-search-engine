from PIL import Image
from sentence_transformers import SentenceTransformer

class MultiModalSearch:
    def __init__(self, model_name="clip-ViT-B-32"):
        self.model = SentenceTransformer(model_name)
    
    def embed_image(self, image_fpath):
        img = Image.open(image_fpath)
        return self.model.encode([img])[0]

def verify_image_embedding(image_fpath):
    ms = MultiModalSearch()
    embedding = ms.embed_image(image_fpath)
    print(f"Embedding shape: {embedding.shape[0]} dimensions")

    